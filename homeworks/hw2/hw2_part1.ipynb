{"cells":[{"cell_type":"markdown","metadata":{"id":"gKTBPBJy-MIp"},"source":["# Instructions\n","\n","*Text Adventure Games* are games in which the player interacts with a rich world only through text. Text adventure games predate computers with graphics. However, in many ways they are more complex than conventional video games because they can involve complicated interactions (e.g., \"build a rope bridge\") that require a fair amount of imagination. Indeed, text adventure games are used as [research testbeds](https://arxiv.org/abs/1909.05398) for natural language processing agents.\n","\n","The canonical text adventure game is [Zork](https://en.wikipedia.org/wiki/Zork), in which the player discover an abandoned underworld realm full of treasure. You can find online playable versions.\n","\n","A text game is made up of individual locations--also called \"rooms\", though they need not be indoor enclosed spaces as the term might imply. The agent can move between rooms and interact with objects by typing in short commands like \"move north\" and \"take lamp\".\n","\n","In this assignment, we will use a special package that implements text worlds for testing agents: [TextWorld-Express](https://github.com/cognitiveailab/TextWorldExpress). Textworld-Express simplifies text worlds in a few ways: it uses a reduced set of text commands, and rooms laid out in a grid.\n","TextWorld-Express also implements a few different game objectives, such as cooking, and searching for coins.\n","TextWorld-Express generates world configurations, so we will need to implement algorithms that are able to complete different game objectives in different world configurations.\n","\n","In this assignment, our agents will play two different games:\n","- Coin Game: a  game in which the agent must search for and pick up a single coin.\n","- Map Reader: a game in which the agent must find a key and return it to a box at the starting location.\n","\n","**We will be implementing the tabular Q-learning algorithm** (as opposed to neural Q-learning).\n","\n","You are prohibited from using any pre-existing python package with built-in graph functions such as Djikstra's algorithm, or Prim's algorithm (e.g., networkx and SciPy). You are prohibited from using any python package that does not come default with Python, except textworld-express, graphviz, and pydot, which are loaded as part of this notebook.\n","\n","**Notes:**\n","- If you break execution of a cell running the game engine, you may put TextWorld-Express in an un-recoverable state. If this happens, you will need to reset your kernel/runtime.\n","- In the Map Reader game, you must use a single search loop (you cannot run a search to the coin and then a separate search to the box). You cannot write specialized code for handling the Map Reader game. You cannot memorize the path to the coin and then reverse it.\n","- In the Map Reader game, you cannot use the map information.\n","- You cannot filter any actions. We've already filtered out the actions that we don't want your agent to have to consider. For example, the \"take map\" action is never helpful, but you must explore it.\n","- ***DO NOT REMOVE ANY COMMENTS THAT HAVE `# EXPORT` IN THEM. THE GRADING SCRIPT USES THESE COMMENTS TO EVALUATE YOUR FUNCTIONS. WE WILL NOT AUDIT SUBMISSIONS TO ADD THESE. IF THE AUTOGRADER FAILS TO RUN DUE TO YOUR MODIFICATION OF THESE COMMENTS, YOU WILL NOT RECEIVE CREDIT.***"]},{"cell_type":"markdown","metadata":{"id":"NMJadvTO-O7z"},"source":["# Install"]},{"cell_type":"markdown","metadata":{"id":"AdVL2D-B-qYC"},"source":["Install the TextWorld-Express engine, and graphviz and pydot for visualization."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30084,"status":"ok","timestamp":1726162493663,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"Q_jkFnjg-C7N","outputId":"280a0361-4304-490b-c6d8-5fcd407a65bb"},"outputs":[],"source":["%pip install gymnasium\n","%pip install textworld-express\n","%pip install graphviz\n","%pip install pydot"]},{"cell_type":"markdown","metadata":{"id":"f0GaE8xE-T6A"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ScAIKmf-Vmm"},"outputs":[],"source":["# export\n","from textworld_express import TextWorldExpressEnv\n","import gymnasium\n","import graphviz\n","import pydot\n","from IPython.display import Image\n","from IPython.display import display\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","import re\n","import os\n","import copy\n","import json\n","import math\n","import random\n","import networkx as nx\n","from itertools import combinations"]},{"cell_type":"markdown","metadata":{"id":"7U9ElZek-y5I"},"source":["# Load a Game"]},{"cell_type":"markdown","metadata":{"id":"Rg_yOwlT_ByQ"},"source":["Set the random seed for repeatablity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIGNj42d-93N"},"outputs":[],"source":["# export\n","SEED = 3"]},{"cell_type":"markdown","metadata":{"id":"_8_CqmGx_GOo"},"source":["Initialize the game environment. `ENV` is a global that encapulates the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRwKCeqK_JOy"},"outputs":[],"source":["ENV = TextWorldExpressEnv(envStepLimit=100)"]},{"cell_type":"markdown","metadata":{"id":"CtJDssBp_Kd_"},"source":["Set the game generator to generate a particular game (coin game or map reader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piAle474_SPG"},"outputs":[],"source":["GAME_TYPE = \"coin\"\n","GAME_PARAMS = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n","ENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)"]},{"cell_type":"markdown","metadata":{"id":"aG_y6anfARuO"},"source":["# TextWorld API Primer"]},{"cell_type":"markdown","metadata":{"id":"aQqFwDKiAVT4"},"source":["This section gives the basics of the TextWorld API."]},{"cell_type":"markdown","metadata":{"id":"Nzxh1WVd_eRs"},"source":["**Reset the game engine.** `ENV.reset()` provides an observation, the text of the current local world, and a data structure called `infos`, which contains a variety of additional information about the current local world."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1726162497080,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"j9o50bHS_m5h","outputId":"12e58ebf-2f9d-4900-9c6f-4fa8cce6b53b"},"outputs":[{"name":"stdout","output_type":"stream","text":["You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n","To the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. \n","{'observation': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'look': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'inventory': 'Inventory (maximum capacity is 2 items): \\n  Your inventory is currently empty.\\n', 'validActions': ['look around', 'close door to west', 'move west', 'open door to south', 'open door to west', 'inventory', 'move south', 'close door to south'], 'scoreRaw': 0.0, 'score': 0.0, 'tasksuccess': False, 'taskfailure': False, 'reward': 0, 'done': False, 'numMoves': 0, 'taskDescription': 'Your task is to search the environment and find the coin.  Once you find the coin, take it.'}\n"]}],"source":["obs, infos = ENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)\n","print(obs)\n","print(infos)"]},{"cell_type":"markdown","metadata":{"id":"TLUOiFkjApoW"},"source":["**Valid actions.** The actions that an agent can perform are part of the `infos` dictionary."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1726162497307,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"9-6Gvk_EAhtW","outputId":"12180cb0-bdf7-4978-b35e-bd41db7794d7"},"outputs":[{"data":{"text/plain":["['look around',\n"," 'close door to west',\n"," 'move west',\n"," 'open door to south',\n"," 'open door to west',\n"," 'inventory',\n"," 'move south',\n"," 'close door to south']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["infos['validActions']"]},{"cell_type":"markdown","metadata":{"id":"aCcPnYUcA15B"},"source":["**Execute an action.** Actions are executed using `ENV.step()`, which returns the new observation for the agent's new state, a reward value, a boolean indicating if the agent has reached the end of the game, and the `infos` for the current state. Here is the code to choose a random valid action and execute it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1726162497308,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"FdKDNjCnBSkb","outputId":"7334c616-6f98-4152-f682-1d449ee93e27"},"outputs":[{"name":"stdout","output_type":"stream","text":["action: move south\n","You can't move there, the door is closed. \n","reward: 0.0\n","done? False\n","infos: {'observation': \"You can't move there, the door is closed. \", 'look': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'inventory': 'Inventory (maximum capacity is 2 items): \\n  Your inventory is currently empty.\\n', 'validActions': ['close door to west', 'move west', 'open door to south', 'inventory', 'close door to south', 'move south', 'open door to west', 'look around'], 'scoreRaw': 0.0, 'score': 0.0, 'tasksuccess': False, 'taskfailure': False, 'reward': 0.0, 'done': False, 'numMoves': 1, 'taskDescription': 'Your task is to search the environment and find the coin.  Once you find the coin, take it.', 'lastActionStr': 'move south'}\n"]}],"source":["# Pick a random action\n","random_action = random.choice(infos['validActions'])\n","print(\"action:\", random_action)\n","# Execute the action\n","obs, reward, done, infos = ENV.step(random_action)\n","print(obs)\n","print(\"reward:\", reward)\n","print(\"done?\", done)\n","print(\"infos:\", infos)"]},{"cell_type":"markdown","metadata":{"id":"2fYCCeL_Bupb"},"source":["The environment knows the \"gold path\", which is the solution sequent of actions. This in not guaranteed to be optimal. *You are prohibited from calling this function as part of your code solutions to the homework problems.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1726162497308,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"-ZcaZ5frCI1r","outputId":"85ebaa69-705e-4ce2-df34-6938c7f077e8"},"outputs":[{"data":{"text/plain":["['look around',\n"," 'open door to south',\n"," 'open door to west',\n"," 'move south',\n"," 'open door to east',\n"," 'move north',\n"," 'move west',\n"," 'move east',\n"," 'move south',\n"," 'move north',\n"," 'move west',\n"," 'move east',\n"," 'move west',\n"," 'move east',\n"," 'move west',\n"," 'move east',\n"," 'move south',\n"," 'move east',\n"," 'take coin']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["ENV.getGoldActionSequence()"]},{"cell_type":"markdown","metadata":{"id":"DEUeySWdB_DE"},"source":["# MDP Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"JtR7eZ6xvS59"},"source":["**Observation Parsing Functions**\n","\n","- `parse_inventory()` attempts to pull the inventory line out of an observation.\n","- `obs_location()` attempts to pull the name of the location of the agent out of the observations.\n","- `hash_state()` converts an observation to a hash code a string of unique numbers.\n","- `parse_things()` attempts to pull out all the objects in an observation.\n","- `parse_doors()` attempts to pull out information about all the doors in an observation. It returns a list of tuples containing `\"name_of_door (direction)\"` and whether it is `'open'` or `'closed'`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6J9x_gDtzyYQ"},"outputs":[],"source":["# export\n","### Pull the inventory items out of the observation text that includes\n","### the inventory (from obs_with_inventory())\n","def parse_inventory(obs):\n","  m = re.search(r'Inventory[a-zA-Z0-9 \\(\\)]*:\\s*([a-zA-Z0-9 \\.\\n]+)', obs)\n","  if m is not None:\n","    if 'empty' not in m.group(1):\n","      return m.group(1).replace('\\n', '')\n","  return 'empty'\n","\n","### Pull the location out of the observation text.\n","def obs_location(obs):\n","  first_sentence = obs.split('.')[0].split(' ')\n","  start = first_sentence.index('the') + 1\n","  return ' '.join(first_sentence[start:])\n","  # return obs.split('.')[0].split(' ')[-1]\n","\n","def hash_state(state):\n","  return str(abs(hash(json.dumps(state))))\n","\n","def parse_things(obs):\n","  things1 = re.findall(r'[yY]ou \\w*\\s*see [aA]? ([a-zA-Z0-9\\- ]+)\\,? that ([a-zA-Z0-9\\-, ]+).', obs)\n","  things2 = re.findall(r'[tT]here is \\w*\\s*([a-zA-Z0-9\\- ]+)\\,? that ([a-zA-Z0-9\\-, ]+).', obs)\n","  things3 = re.findall(r'[tT]here is \\w*\\s*([a-zA-Z0-9\\- ]+)\\.', obs)\n","  things3 = list(filter(lambda s:'that' not in s, things3))\n","  things4 = re.findall(r'[yY]ou \\w*\\s*see a ([a-zA-Z0-9\\- ]+)\\.', obs)\n","  things4 = list(filter(lambda s:'door' not in s and 'that' not in s, things4))\n","  return list(map(lambda x: list(x) if type(x) is tuple else x,\n","                  things1 + things2 + things3 + things4))\n","\n","def parse_doors(obs):\n","  sentences = obs.split('.')\n","  doors = []\n","  dirs = re.compile('west|east|south|north')\n","  for sentence in sentences:\n","    m_open = re.search(r'open ([a-z\\- ]*door)', sentence)\n","    m_closed = re.search(r'closed ([a-z\\- ]*door)', sentence)\n","    dir = dirs.search(sentence.lower())\n","    if dir is not None:\n","      if m_open is not None:\n","        doors.append((m_open[1] + ' (' + dir[0] + ')', 'open'))\n","      elif m_closed is not None:\n","        doors.append((m_closed[1] + ' (' + dir[0] + ')', 'closed'))\n","  return doors"]},{"cell_type":"markdown","metadata":{"id":"ZZWNNrycW83G"},"source":["**Environment Interaction Functions**\n","\n","These functions allow the agent to interact with the environment in a slightly more friendly way than the default `env.reset()` and `env.step()` functions. They wrap those funtions and do some processing on the data to bundle it in a way that will be easier to work with.\n","\n","`reset_mdp()` takes an environment (e.g., `ENV`) and returns a dictionary containing the observation, inventory, and valid actions. For example:\n","```\n","{'observation': 'You are in the kitchen...',\n"," 'inventory': 'Inventory (maximum capacity is 2 items):...',\n"," 'valid actions': ['move north', 'move south', 'take coin'...]\n","}\n","```\n","\n","`do_action_mdp()` takes the name of an action and a pointer to the environment. It returns the state that results from executing the action. It returns 4 values:\n","- observation: a string\n","- reward: a floating point number\n","- termination: a boolean indicating whether the episode has ended\n","- infos: a dictionary containing observation, inventory, and valid actions (as above).\n","\n","**Use these functions instead of `env.reset()` and `env.step()`.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tux5anN-B7ib"},"outputs":[],"source":["# export\n","def reset_mdp(env):\n","  obs, infos = env.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)\n","  valids = infos['validActions']\n","  valids.remove('inventory')\n","  valids.remove('look around')\n","  inv = infos['inventory']\n","  modified_obs = obs_with_inventory(infos['look'], inv)\n","  # return make_state_mdp(infos['look'], parse_inventory(infos['inventory'])), valids\n","  return {'observation': infos['look'],\n","          'inventory': infos['inventory'],\n","          'valid actions': valids}\n","\n","\n","def do_action_mdp(action, env):\n","  obs, reward, done, infos = env.step(action)\n","  #obs_look, reward_look, done_look, infos_look = env.step('look around')\n","  valid_actions = infos['validActions']\n","  valid_actions.remove('inventory')\n","  valid_actions.remove('look around')\n","  # return make_state_mdp(infos['look'], parse_inventory(infos['inventory'])), reward, done, valid_actions\n","  return infos['look'], reward, done, {'observation': infos['look'],\n","                                       'inventory': infos['inventory'],\n","                                       'valid actions': valid_actions}"]},{"cell_type":"markdown","metadata":{"id":"nJzznPc_wswe"},"source":["# Important Notes for this Assignment\n","\n","\n","*   A successful episode from the MDP will give a reward of 1.0\n","*   A partially successful episode from an MDP environment will give a reward of 0.5\n","*   If you increase NUM_EPISODES too high, it will take too long in the autograder.\n","*   We will be checking for hard coded values / outputs, so please don't take any shortcuts.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xVKlX5m4CKxc"},"source":["# Implement Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"u8gOs8DSrTa4"},"source":["**Step 1.** Implement the `q_learning()` function. This function takes the following parameters:\n","- env: a pointer to the environment (`ENV`).\n","- num_episodes: the number of episodes to run before termination of the entire algorithm.\n","- threshold: the number of steps in an episode before terminating a single episode.\n","- learning_rate: a number between 0 and 1 controlling how fast the policy is allowed to change.\n","- gamma: the Bellman equation horizon parameter (between 0 and 1).\n","- epsilon: (optional) if epsilon greedy is implemented, this number (0..1) determines the ratio of random to policy-guided actions. A value of 1.0 indicates purely random, and a value of 0.0 indicates purely on-policy.\n","\n","The `q_learning()` algorithm should return a single value: the policy. The policy will be a dictionary-of-dictionaries where the outermost dictionary has a key for each state visited. Each state points to a separate inner dictionary where the keys are actions and the values are q-values. For example:\n","```\n","{state1: {'move north': 0.1,\n","          'move south': 0.0,\n","          'move east': 0.8,\n","          'move west': 0.4},\n"," state2: {'move north': 0.01,\n","          'take coin': 1.0,\n","          'move east': 0.05,\n","          'move west': 0.2},\n"," ...\n","}\n","```\n","\n","You will interact with the environment through `reset_mdp()` and `do_action_mdp()`. You can choose your own representation for states, though it must be hashable (string, number, tuple, etc.).\n","\n","We recommend you track your algorithm's performance by tracking the total reward of each episode, and the number of step in each episode (fewer is better). If you use purely random action selection, you will see a lot of variance in your total episode reward. If you implement epsilon-greedy, you will see a trend toward more consistent achievement of maximum reward as episode number increases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVpU7tLJCXnu"},"outputs":[],"source":["# export\n","# please print out obs and inventory and find a way to combine them together\n","# this is much simpler to implement than it seems (one line of code)\n","def obs_with_inventory(obs, inv):\n","  ### YOUR CODE BELOW HERE\n","  raise NotImplementedError\n","  return None\n","  ### YOUR CODE ABOVE HERE\n","\n","def q_learning(env, num_episodes, threshold, learning_rate, gamma, epsilon = 1.0):\n","  # Set up q-table\n","  q_table = {}\n","  ### YOUR CODE BELOW HERE\n","  raise NotImplementedError\n","  ### YOUR CODE ABOVE HERE\n","  return q_table"]},{"cell_type":"markdown","metadata":{"id":"js7x9FBAr_0e"},"source":["**Step 2.** Set the parameters for your q-learning algorithm. You can change these values.\n","\n","**These might need to be changed from the default values. These variables are just for the simple test below. The autograder will use the variables you set in `set_parameters` below.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1paMMHQnCV6L"},"outputs":[],"source":["NUM_EPISODES = 100\n","THRESHOLD = 25\n","LEARNING_RATE = 0.5\n","GAMMA = 0.5\n","EPSILON = 1.0"]},{"cell_type":"markdown","metadata":{"id":"Ih1dGqD2Yqyk"},"source":["Run the q-learning algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCdoOwDaGjjf"},"outputs":[],"source":["q_table = q_learning(ENV,\n","                     num_episodes = NUM_EPISODES,\n","                     threshold = THRESHOLD,\n","                     learning_rate = LEARNING_RATE,\n","                     gamma = GAMMA,\n","                     epsilon = EPSILON)"]},{"cell_type":"markdown","metadata":{"id":"-z2k4EkrGaMm"},"source":["# Implement Code to Run a Policy"]},{"cell_type":"markdown","metadata":{"id":"_j5OrjrTtxWM"},"source":["**Step 3.** Implement code to run the policy. This function takes the following parameters:\n","- q_table: your q-table, as specified in step 1.\n","- env: pointer to the environment (e.g., `ENV`).\n","- threshold: the maximum number of steps to take before terminating.\n","\n","Your function should run a single episode from the initial state and return:\n","- A list of actions taken during the episode (e.g., `[act_1, act_2, ... act_n]`).\n","- The total sum reward of all actions taken as a float.\n","\n","Your function will interact with the environment through `reset_mdp()` and `do_action_mdp()`. Be sure to reset the environment before running, and terminate the episode if `do_action_mdp()` indicates the episode has terminated.\n","\n","**IMPORTANT:** It is possible that your agent encounters states in run_policy that it never saw when it was building the Q_table. Please handle this by choosing a random action from the available actions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jJCxMBTGdv9"},"outputs":[],"source":["# export\n","def run_policy(q_table, env, threshold = 25):\n","  actions = [] # Store the entire sequence of actions here\n","  total_reward = 0.0 # Store the total sum reward of all actions executed here\n","  ### YOUR CODE BELOW HERE\n","  raise NotImplementedError\n","  ### YOUR CODE ABOVE HERE\n","  return actions, total_reward"]},{"cell_type":"markdown","metadata":{"id":"goBOBYvBYzeR"},"source":["**Step 4.** Set the threshold value for episode length during policy execution (test time threshold)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpK_2wuMuZ7d"},"outputs":[],"source":["# export\n","TEST_THRESHOLD = 25"]},{"cell_type":"markdown","metadata":{"id":"lWOGP5aWY59n"},"source":["**Step 5.** Run the policy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1726162497495,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"vhrGyDGBGrMP","outputId":"441b536d-2e42-47e9-9f70-d0f6d7da290b"},"outputs":[{"name":"stdout","output_type":"stream","text":["plan: []\n","Total reward: 0.0\n"]}],"source":["plan, total_reward = run_policy(q_table, ENV, threshold = TEST_THRESHOLD)\n","print(\"plan:\", plan)\n","print(\"Total reward:\", total_reward)"]},{"cell_type":"markdown","metadata":{"id":"0cBfmUDG1XHk"},"source":["# New Environment: Stochastic Actions"]},{"cell_type":"markdown","metadata":{"id":"pLeYTiUVY9Tj"},"source":["The following creates a new type of environment called `StochasticTextWorldExpressEnv`. This environment is the same as the previous environment type, except that some percentage of the time, the action that the agent chooses is not executed and a randomly chosen action is executed instead.\n","\n","When the environment is created the percentage of action randomness (between 0 and 1) is set, where 0.0 means no randomness, and 1.0 means that actions are executed purely randomly.\n","\n","Otherwise, this environment works the same as previously.\n","\n","If you set the global variable `ENV_VERBOSE = True` then the environment will print the action that is executed, regardless of whether it is the one selected or a random action. This is for debugging purposes only.\n","\n","**NOTE:** The agent is never able to know whether the action it chose was executed or if a different action was executed.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-hc6VnL793a"},"outputs":[],"source":["NEVER_PICK_ACTIONS = set(['look around', 'inventory'])\n","ENV_VERBOSE = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsQWZj0H1WeS"},"outputs":[],"source":["class StochasticTextWorldExpressEnv(TextWorldExpressEnv):\n","\n","  def __init__(self, serverPath=None, envStepLimit=100, stochasticity = 0.0):\n","    # Call the super constructor\n","    super().__init__(serverPath, envStepLimit)\n","    # Store the valid actions and stochasticity\n","    self.valid_actions = []\n","    self.stochasticity = stochasticity\n","\n","  def reset(self, seed=None, gameFold=None, gameName=None, gameParams=None, generateGoldPath=False):\n","    # Call the super method\n","    observation, infos = super().reset(seed, gameFold, gameName, gameParams, generateGoldPath)\n","    # Update the valid actions\n","    self.valid_actions = infos['validActions']\n","    return observation, infos\n","\n","  def step(self, action:str):\n","    # If a random value is less than the stochasticity target, choose a random action\n","    if random.random() < self.stochasticity:\n","      temp_valids = copy.deepcopy(self.valid_actions)\n","      # Remove inventory and look around from valid actions to choose from\n","      temp_valids = list(set(self.valid_actions).difference(NEVER_PICK_ACTIONS))\n","      # Pick a random action from whatever remains\n","      action = random.choice(temp_valids)\n","    # If debugging flag is on, print the action that will be executed\n","    if ENV_VERBOSE:\n","      print(\"[[action]]:\", action)\n","    # Call the super class with either the action passed in or the randomly chosen one\n","    observation, reward, isCompleted, infos = super().step(action)\n","    # Update the valid actions\n","    self.valid_actions = infos['validActions']\n","    return observation, reward, isCompleted, infos"]},{"cell_type":"markdown","metadata":{"id":"lKV4lUC1agso"},"source":["New environments must be registered through the Gymnasium API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFWWPDS35bMn"},"outputs":[],"source":["gymnasium.register(id='TextWorldExpress-StochasticTextWorldExpressEnv-v0',\n","                   entry_point='__main__:StochasticTextWorldExpressEnv')"]},{"cell_type":"markdown","metadata":{"id":"rvmTtxrwalKn"},"source":["Create the new environment type."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7RMiGJoc62eI"},"outputs":[],"source":["SENV = StochasticTextWorldExpressEnv(envStepLimit=100, stochasticity=0.25)"]},{"cell_type":"markdown","metadata":{"id":"hbB48O0Eang4"},"source":["Create a game with this environment type (same as before)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOkAnt267ETX"},"outputs":[],"source":["GAME_TYPE = \"coin\"\n","GAME_PARAMS = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n","SENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)"]},{"cell_type":"markdown","metadata":{"id":"CmdycTPAataX"},"source":["Reset the environment (same as before)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1726162499347,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"hwt58q1_7OZ3","outputId":"4a3fcc67-dd26-42c8-a463-f812d705257b"},"outputs":[{"name":"stdout","output_type":"stream","text":["You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n","To the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. \n","{'observation': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'look': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'inventory': 'Inventory (maximum capacity is 2 items): \\n  Your inventory is currently empty.\\n', 'validActions': ['look around', 'close door to west', 'move west', 'open door to south', 'open door to west', 'inventory', 'move south', 'close door to south'], 'scoreRaw': 0.0, 'score': 0.0, 'tasksuccess': False, 'taskfailure': False, 'reward': 0, 'done': False, 'numMoves': 0, 'taskDescription': 'Your task is to search the environment and find the coin.  Once you find the coin, take it.'}\n"]}],"source":["obs, infos = SENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)\n","print(obs)\n","print(infos)"]},{"cell_type":"markdown","metadata":{"id":"M-4ZrEcXawLI"},"source":["Execute a step. If `ENV_VERBOSE=True` then the action actually executed will be printed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1726162499348,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"bYWtb6tG7coF","outputId":"523d6a08-6eb8-4d20-ee1f-2691e08d42f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["You open the sliding patio door, revealing the backyard. \n","0.0\n","False\n","{'observation': 'You open the sliding patio door, revealing the backyard. ', 'look': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nThrough an open sliding patio door, to the South you see the backyard. To the West you see a closed frosted-glass door. ', 'inventory': 'Inventory (maximum capacity is 2 items): \\n  Your inventory is currently empty.\\n', 'validActions': ['close door to west', 'move west', 'open door to south', 'inventory', 'close door to south', 'move south', 'open door to west', 'look around'], 'scoreRaw': 0.0, 'score': 0.0, 'tasksuccess': False, 'taskfailure': False, 'reward': 0.0, 'done': False, 'numMoves': 1, 'taskDescription': 'Your task is to search the environment and find the coin.  Once you find the coin, take it.', 'lastActionStr': 'open door to south'}\n"]}],"source":["obs, reward, done, infos = SENV.step('open door to south')\n","print(obs)\n","print(reward)\n","print(done)\n","print(infos)"]},{"cell_type":"markdown","metadata":{"id":"yktsPacua4nu"},"source":["Train in the stochastic Text World environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2MoSrPaESpM"},"outputs":[],"source":["q_table = q_learning(SENV,\n","                     num_episodes = NUM_EPISODES,\n","                     threshold = THRESHOLD,\n","                     learning_rate = LEARNING_RATE,\n","                     gamma = GAMMA,\n","                     epsilon = EPSILON)"]},{"cell_type":"markdown","metadata":{"id":"zEGhWdYma8D-"},"source":["Test the policy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1726162499546,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"juNfb4q8EXAa","outputId":"a83c3325-82dd-42ba-b511-18f3188acf1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["plan: []\n","total reward: 0.0\n"]}],"source":["plan, total_reward = run_policy(q_table, SENV, threshold = TEST_THRESHOLD)\n","print(\"plan:\", plan)\n","print(\"total reward:\", total_reward)"]},{"cell_type":"markdown","metadata":{"id":"T4sT-9IcM7Xs"},"source":["# New Environment: Negative Reward"]},{"cell_type":"markdown","metadata":{"id":"gXe7_ZhkQq9I"},"source":["The following creates a new type of environment called `PunishmentTextWorldExpressEnv`. This environment is the same as the previous environment type, except that the agent receives negative reward when it performs actions that are illegal or do not change the world state. For example, trying to close a door that is already closed, or move in a direction that is illegal.\n","\n","Otherwise, this environment works the same as previously."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NW6uZ2Z8M-b9"},"outputs":[],"source":["class PunishmentTextWorldExpressEnv(TextWorldExpressEnv):\n","\n","  def __init__(self, serverPath=None, envStepLimit=100, punishment = 0.0):\n","    # Call the super constructor\n","    super().__init__(serverPath, envStepLimit)\n","    # Store the punishment\n","    self.punishment = punishment\n","    # Store the previous observation\n","    self.previous_observation = None\n","\n","  def step(self, action:str):\n","    # Call the super method\n","    observation, reward, isCompleted, infos = super().step(action)\n","    # If the current look is the same as the previous look, then we have performed an illegal action\n","    if infos['look'] == self.previous_observation:\n","      reward = self.punishment\n","    # Store the previous observation\n","    self.previous_observation = infos['look']\n","    return observation, reward, isCompleted, infos"]},{"cell_type":"markdown","metadata":{"id":"GKMjQ_cF5_1C"},"source":["New environments must be registered through the Gymnasium API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIyZMwA3Ox4H"},"outputs":[],"source":["gymnasium.register(id='TextWorldExpress-PunishmentTextWorldExpressEnv-v0',\n","                   entry_point='__main__:PunishmentTextWorldExpressEnv')"]},{"cell_type":"markdown","metadata":{"id":"QH1p6rTm6NkA"},"source":["Create the new environment type."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9elwI9UPKEL"},"outputs":[],"source":["PENV = PunishmentTextWorldExpressEnv(envStepLimit=100, punishment=-1.0)"]},{"cell_type":"markdown","metadata":{"id":"S2uUZ9496Rm3"},"source":["Create a game with this environment type (same as before)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjGRhjzCPZGQ"},"outputs":[],"source":["GAME_TYPE = \"coin\"\n","GAME_PARAMS = \"numLocations=5,includeDoors=1,numDistractorItems=0\"\n","PENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)"]},{"cell_type":"markdown","metadata":{"id":"hE9A43186VcL"},"source":["Reset the environment (same as before)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1726162502438,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"9-cKjWIdPcXU","outputId":"79e619ff-3fc5-4f63-dc25-20903d73a94a"},"outputs":[{"name":"stdout","output_type":"stream","text":["You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \n","To the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. \n","{'observation': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'look': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nTo the South you see a closed sliding patio door. To the West you see a closed frosted-glass door. ', 'inventory': 'Inventory (maximum capacity is 2 items): \\n  Your inventory is currently empty.\\n', 'validActions': ['look around', 'close door to west', 'move west', 'open door to south', 'open door to west', 'inventory', 'move south', 'close door to south'], 'scoreRaw': 0.0, 'score': 0.0, 'tasksuccess': False, 'taskfailure': False, 'reward': 0, 'done': False, 'numMoves': 0, 'taskDescription': 'Your task is to search the environment and find the coin.  Once you find the coin, take it.'}\n"]}],"source":["obs, infos = PENV.reset(seed=SEED, gameFold=\"train\", generateGoldPath=True)\n","print(obs)\n","print(infos)"]},{"cell_type":"markdown","metadata":{"id":"a_91pXP36ZYh"},"source":["Execute a step."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1726162502438,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"tkFUVKHxPeom","outputId":"c5274381-2bef-43ea-9cb5-80921f7e1bb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["You open the sliding patio door, revealing the backyard. \n","0.0\n","False\n","{'observation': 'You open the sliding patio door, revealing the backyard. ', 'look': 'You are in the kitchen. In one part of the room you see a stove. There is also an oven. You also see a fridge that is closed. In another part of the room you see a counter, that has nothing on it. In one part of the room you see a kitchen cupboard that is closed. There is also a cutlery drawer that is closed. You also see a trash can that is closed. In another part of the room you see a dishwasher that is closed. In one part of the room you see a dining chair, that has nothing on it. \\nThrough an open sliding patio door, to the South you see the backyard. To the West you see a closed frosted-glass door. ', 'inventory': 'Inventory (maximum capacity is 2 items): \\n  Your inventory is currently empty.\\n', 'validActions': ['close door to west', 'move west', 'open door to south', 'inventory', 'close door to south', 'move south', 'open door to west', 'look around'], 'scoreRaw': 0.0, 'score': 0.0, 'tasksuccess': False, 'taskfailure': False, 'reward': 0.0, 'done': False, 'numMoves': 1, 'taskDescription': 'Your task is to search the environment and find the coin.  Once you find the coin, take it.', 'lastActionStr': 'open door to south'}\n"]}],"source":["obs, reward, done, infos = PENV.step('open door to south')\n","print(obs)\n","print(reward)\n","print(done)\n","print(infos)"]},{"cell_type":"markdown","metadata":{"id":"knWwRNQV6IyA"},"source":["Train in the punishment Text World environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K9GoERC6cmS"},"outputs":[],"source":["q_table = q_learning(PENV,\n","                     num_episodes = NUM_EPISODES,\n","                     threshold = THRESHOLD,\n","                     learning_rate = LEARNING_RATE,\n","                     gamma = GAMMA,\n","                     epsilon = EPSILON)"]},{"cell_type":"markdown","metadata":{"id":"IDLjkAz66DNm"},"source":["Test the policy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1726162502651,"user":{"displayName":"Yash Gupta","userId":"17184721603007691730"},"user_tz":240},"id":"c1oehdR96fsu","outputId":"315cacd7-fcb1-4ba0-dd80-0578cfb2d5e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["plan: []\n","total reward: 0.0\n"]}],"source":["plan, total_reward = run_policy(q_table, SENV, threshold = TEST_THRESHOLD)\n","print(\"plan:\", plan)\n","print(\"total reward:\", total_reward)"]},{"cell_type":"markdown","metadata":{"id":"ywMkXNMwQsgZ"},"source":["# Testing Suite"]},{"cell_type":"markdown","metadata":{"id":"gR9ZHmgw8UFa"},"source":["This function will run all environments, all game types, all game parameters, and all seeds."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLy0jJ9gRA0L"},"outputs":[],"source":["def run_all(environments, games, seeds):\n","  global ENV, GAME_TYPE, GAME_PARAMS, SEED\n","  # Results will contain a key (env type, game type, game params, seed) and values will be plans and total_rewards\n","  results = {}\n","  # Iterate through all environments given\n","  for env in environments:\n","    # set global environment\n","    ENV = env\n","    # Iterate through all game types, the keys of the games dict\n","    for game_type in games:\n","      # Set the global game type\n","      GAME_TYPE = game_type\n","      # Iterate through all game parameters for the given game type in game dict\n","      for params in games[game_type]:\n","        # set the global game params\n","        GAME_PARAMS = params\n","        # load the environment\n","        ENV.load(gameName=GAME_TYPE, gameParams=GAME_PARAMS)\n","        # Iterate through all seeds\n","        for seed in seeds:\n","          print(f\"TESTING {type(ENV)}, {GAME_TYPE}, {GAME_PARAMS}, {seed}\")\n","          # set the global seed\n","          SEED = seed\n","          # Run the q learner and get the policy\n","          q_table = q_learning(ENV,\n","                               num_episodes = NUM_EPISODES,\n","                               threshold = THRESHOLD,\n","                               learning_rate = LEARNING_RATE,\n","                               gamma = GAMMA,\n","                               epsilon = EPSILON)\n","          # run the policy to get the plan\n","          plan, total_reward = run_policy(q_table, ENV, threshold = TEST_THRESHOLD)\n","          # Store the plan in the results\n","          results[(type(ENV), GAME_TYPE, GAME_PARAMS, SEED)] = (plan, total_reward)\n","  return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeBUqaKWbBQj"},"outputs":[],"source":["seeds = list(range(5))\n","environments = [TextWorldExpressEnv(envStepLimit=100),\n","                StochasticTextWorldExpressEnv(envStepLimit=100, stochasticity=0.25),\n","                PunishmentTextWorldExpressEnv(envStepLimit=100, punishment=-1.0)]\n","games = {'coin':      ['numLocations=5,includeDoors=1,numDistractorItems=0',\n","                       'numLocations=6,includeDoors=1,numDistractorItems=0',\n","                       'numLocations=7,includeDoors=1,numDistractorItems=0',\n","                       'numLocations=10,includeDoors=1,numDistractorItems=0'],\n","         'mapreader': ['numLocations=5,maxDistanceApart=3,includeDoors=0,maxDistractorItemsPerLocation=0',\n","                       'numLocations=8,maxDistanceApart=4,includeDoors=0,maxDistractorItemsPerLocation=0',\n","                       'numLocations=11,maxDistanceApart=5,includeDoors=0,maxDistractorItemsPerLocation=0',\n","                       'numLocations=15,maxDistanceApart=8,includeDoors=0,maxDistractorItemsPerLocation=0']}"]},{"cell_type":"markdown","metadata":{"id":"NaZoBv5v8Qa9"},"source":["Set parameters. Do not alter this cell outside of the changing the numeric values.\n","\n","**You might need to change these parameters to get a good result on the harder environments**\n","\n","Please note that increasing `NUM_EPISODES` will result in an increase in time to run the cell below. Your code MUST be completed within the time limit (20min) set by Gradescope."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DaxveUb8PQv"},"outputs":[],"source":["# export\n","def set_parameters():\n","    global NUM_EPISODES, THRESHOLD, LEARNING_RATE, GAMMA, EPSILON, TEST_THRESHOLD\n","    NUM_EPISODES = 100\n","    THRESHOLD = 25\n","    LEARNING_RATE = 0.5\n","    GAMMA = 0.5\n","    EPSILON = 1.0\n","    TEST_THRESHOLD = 25\n","\n","    return {\n","      'NUM_EPISODES': NUM_EPISODES,\n","      'THRESHOLD': THRESHOLD,\n","      'LEARNING_RATE': LEARNING_RATE,\n","      'GAMMA': GAMMA,\n","      'EPSILON': EPSILON,\n","      'TEST_THRESHOLD': TEST_THRESHOLD\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9dAzWLerjKI"},"outputs":[],"source":["set_parameters()"]},{"cell_type":"markdown","metadata":{"id":"PzB7i03p8dij"},"source":["Run all tests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xxm2giWM8fcJ"},"outputs":[],"source":["results = run_all(environments, games, seeds)"]},{"cell_type":"markdown","metadata":{"id":"acQaep88V13E"},"source":["# Grading"]},{"cell_type":"markdown","metadata":{"id":"kDcg7IENXtj1"},"source":["Grading will consist of testing all three environments (regular, stochastic, punishment), two games per environment (coin, mapreader), four sets of parameters per game, and five seeds. There will be a total of 120 tests: 1 point for each correct plan per algorithm.\n","\n","Please note that since these environments can be stochastic, we will give some leeway with the results for plans and reward values. The final grade for Part 1 will be\n","\n","**Grading:**\n","\n","Maximum total points: 50\n","\n","| # correct plan | Score |\n","|----------|-------|\n","| >= 110   |  50   |\n","| 109      |  49   |\n","| 108      |  48   |\n","| ...      |  ...  |\n","| 101      |  41   |\n","| 100      |  40   |\n","| 99       |   39  |\n","| ...      | ... |\n","|  <= 60   |   0   |\n","\n","\n","*Note* Grading will be conducted by visual inspection and autograder on the Gradescope. We will compare your plans and reward to our rubric/reference implementations. We will add cells to your notebook at grading time to load and test our hidden world configuration files.\n","\n","*Note* We will visually inspect the entire notebook to check if your algorithm implementations include details that are inconsistent with the assignment (e.g., hard-coding values or actions to pass tests) and to make sure no cells were altered to provide unearned grading results."]},{"cell_type":"markdown","metadata":{"id":"sA0XSuitsCP5"},"source":["# Submission\n","\n","Upload this notebook with the name `hw2_part1.ipynb` file to Gradescope. The autograder will only run successfully if your file is named this way. \n","\n","We've added appropriate comments to the top of certain cells for the autograder to export (`# export`). You do NOT have to do anything (e.g. remove print statements) to cells we have provided - anything related to those have been handled for you. You are responsible for ensuring your own code has no syntax errors or unnecessary print statements. You ***CANNOT*** modify the export comments at the top of the cells, or the autograder will fail to run on your submission.\n","\n","You should ***not*** add any cells to the notebook when submitting. You're welcome to add any code as you need to extra cells when testing, but you must remove them when submitting. \n","\n","If you identify an issue with the autograder, please feel free to reach out to us on Piazza, or email bok004@ucsd.edu."]}],"metadata":{"colab":{"collapsed_sections":["7U9ElZek-y5I","aG_y6anfARuO","DEUeySWdB_DE"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
